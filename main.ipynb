{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karll\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU support: False\n",
      "os              : Windows-10-10.0.22621-SP0\n",
      "python          : 3.9.13\n",
      "tsai            : 0.3.5\n",
      "fastai          : 2.7.11\n",
      "fastcore        : 1.5.28\n",
      "torch           : 1.13.1+cpu\n",
      "cpu cores       : 6\n",
      "threads per cpu : 2\n",
      "RAM             : 15.9 GB\n",
      "GPU memory      : [6.0] GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Preprocessing.preprocessing import preprocessing\n",
    "import time\n",
    "import torch\n",
    "from tsai.all import *\n",
    "\n",
    "print('GPU support:', torch.cuda.is_available())\n",
    "computer_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimizer.optimizer import optimize_model\n",
    "\n",
    "df = pd.read_csv('Data\\Stock\\StockBars\\MSFT_Minute')\n",
    " \n",
    "preprocessing_params = {\n",
    "    'df': df[:1000],\n",
    "    'lag': 1,\n",
    "    'dif_all': True,\n",
    "    'train_size': 0.8,\n",
    "    'TSAI': True,\n",
    "    'CLF': True,\n",
    "    'index': None,\n",
    "    'data': \"alpacca\",\n",
    "    'buckets': 1\n",
    "}\n",
    "\n",
    "model_type = 'tst_class'\n",
    "\n",
    "opti = True\n",
    "if opti:\n",
    "    optimize_model(model_type=model_type, preprocessing_params=preprocessing_params, n_trials=2)\n",
    "\n",
    "results_df = pd.read_csv(f\"models/{model_type}/{model_type}_hyperparameters_results.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-22 10:44:33,784]\u001b[0m A new study created in memory with name: no-name-9ae5c0ab-10f0-4685-bd68-08b38c3d00ec\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               datetime       open       high        low      close  volume  \\\n",
      "23  2021-02-16 12:49:00  133.52499  133.52991  133.31000  133.37849  183270   \n",
      "24  2021-02-16 12:50:00  133.38000  133.41000  133.31000  133.40759   95343   \n",
      "25  2021-02-16 12:51:00  133.39500  133.42999  133.36000  133.36501   77483   \n",
      "26  2021-02-16 12:52:00  133.36000  133.42999  133.36000  133.42990  125363   \n",
      "27  2021-02-16 12:53:00  133.42500  133.49500  133.42500  133.49500   95862   \n",
      "28  2021-02-16 12:54:00  133.49500  133.50000  133.46730  133.47501  105994   \n",
      "29  2021-02-16 12:55:00  133.48000  133.48080  133.44000  133.46001   77085   \n",
      "30  2021-02-16 12:56:00  133.47000  133.50000  133.44930  133.44930   86462   \n",
      "31  2021-02-16 12:57:00  133.44000  133.46989  133.41000  133.42000   72956   \n",
      "32  2021-02-16 12:58:00  133.41150  133.53000  133.41000  133.50999   63934   \n",
      "33  2021-02-16 12:59:00  133.50999  133.53999  133.47501  133.47501   63385   \n",
      "34  2021-02-16 13:00:00  133.47501  133.50079  133.46010  133.47000  132727   \n",
      "35  2021-02-16 13:01:00  133.38499  133.41991  133.38000  133.41499   30496   \n",
      "36  2021-02-16 13:02:00  133.41000  133.43919  133.39000  133.41000   92994   \n",
      "37  2021-02-16 13:03:00  133.41000  133.49001  133.41000  133.48500   68466   \n",
      "38  2021-02-16 13:04:00  133.48010  133.53000  133.47000  133.51500   80027   \n",
      "39  2021-02-16 13:05:00  133.51900  133.54770  133.49001  133.53000   67969   \n",
      "40  2021-02-16 13:06:00  133.54930  133.56000  133.50000  133.53000  130827   \n",
      "41  2021-02-16 13:07:00  133.53500  133.55949  133.48000  133.55949   81971   \n",
      "42  2021-02-16 13:08:00  133.55991  133.56000  133.48500  133.48500   99887   \n",
      "43  2021-02-16 13:09:00  133.49071  133.53999  133.49001  133.51080   71880   \n",
      "44  2021-02-16 13:10:00  133.51500  133.55000  133.50500  133.51930   80295   \n",
      "45  2021-02-16 13:11:00  133.51930  133.55000  133.45000  133.45000   97488   \n",
      "46  2021-02-16 13:12:00  133.45990  133.49800  133.42999  133.42999   88887   \n",
      "47  2021-02-16 13:13:00  133.42000  133.45000  133.37000  133.37500   83539   \n",
      "\n",
      "    MACD_line  MACD_diff  MACD_signal        RSI      bb_bbm      bb_bbh  \\\n",
      "23  -0.035276   0.000666    -0.035942  37.989993  133.523649  133.683115   \n",
      "24  -0.037829  -0.001258    -0.036571  40.422204  133.513028  133.673700   \n",
      "25  -0.042750  -0.004119    -0.038631  38.119312  133.501029  133.668151   \n",
      "26  -0.040034  -0.000936    -0.039099  43.301069  133.493774  133.660017   \n",
      "27  -0.031695   0.004936    -0.036631  47.907618  133.490024  133.652560   \n",
      "28  -0.026703   0.006618    -0.033321  46.681716  133.483774  133.638330   \n",
      "29  -0.023844   0.006318    -0.030162  45.756885  133.473524  133.603957   \n",
      "30  -0.022242   0.005280    -0.027522  45.085567  133.468490  133.594421   \n",
      "31  -0.023208   0.002876    -0.026084  43.257923  133.461160  133.580277   \n",
      "32  -0.015730   0.006903    -0.022633  49.832658  133.458910  133.572421   \n",
      "33  -0.012950   0.006455    -0.019405  47.577006  133.459160  133.572790   \n",
      "34  -0.011102   0.005535    -0.016637  47.254531  133.453160  133.549946   \n",
      "35  -0.014304   0.001555    -0.015860  43.821584  133.448659  133.543741   \n",
      "36  -0.016859  -0.000666    -0.016193  43.519697  133.445909  133.542116   \n",
      "37  -0.011965   0.002818    -0.014784  49.070586  133.451194  133.543727   \n",
      "38  -0.005538   0.006164    -0.011702  51.094354  133.457445  133.549484   \n",
      "39   0.000615   0.008211    -0.007596  52.096217  133.461444  133.558650   \n",
      "40   0.005159   0.008503    -0.003345  52.096217  133.463214  133.563994   \n",
      "41   0.010986   0.009554     0.001432  54.142637  133.465239  133.571877   \n",
      "42   0.008661   0.004819     0.003842  48.620080  133.463239  133.566775   \n",
      "43   0.009023   0.003454     0.005569  50.462273  133.469855  133.567633   \n",
      "44   0.009871   0.002868     0.007003  51.070669  133.475440  133.571092   \n",
      "45   0.004327  -0.001784     0.006111  46.201070  133.479690  133.561955   \n",
      "46  -0.001584  -0.005130     0.003546  44.899833  133.479694  133.561948   \n",
      "47  -0.010693  -0.009493    -0.001200  41.517266  133.473695  133.567327   \n",
      "\n",
      "        bb_bbl  bb_bbhi  bb_bbli       CMF  \n",
      "23  133.364183      0.0      0.0 -0.239991  \n",
      "24  133.352357      0.0      0.0 -0.175580  \n",
      "25  133.333907      0.0      0.0 -0.212679  \n",
      "26  133.327531      0.0      0.0 -0.104881  \n",
      "27  133.327487      0.0      0.0 -0.061825  \n",
      "28  133.329217      0.0      0.0 -0.097802  \n",
      "29  133.343092      0.0      0.0 -0.137925  \n",
      "30  133.342558      0.0      0.0 -0.131523  \n",
      "31  133.342042      0.0      0.0 -0.158183  \n",
      "32  133.345398      0.0      0.0 -0.118713  \n",
      "33  133.345530      0.0      0.0 -0.066826  \n",
      "34  133.356374      0.0      0.0 -0.144444  \n",
      "35  133.353578      0.0      0.0 -0.104378  \n",
      "36  133.349703      0.0      0.0 -0.091744  \n",
      "37  133.358662      0.0      0.0  0.004452  \n",
      "38  133.365405      0.0      0.0  0.054982  \n",
      "39  133.364239      0.0      0.0  0.051067  \n",
      "40  133.362435      0.0      0.0 -0.000775  \n",
      "41  133.358601      0.0      0.0  0.038025  \n",
      "42  133.359704      0.0      0.0  0.004664  \n",
      "43  133.372077      0.0      0.0  0.038103  \n",
      "44  133.379789      0.0      0.0 -0.031905  \n",
      "45  133.397425      0.0      0.0 -0.049551  \n",
      "46  133.397441      0.0      0.0 -0.177236  \n",
      "47  133.380062      0.0      1.0 -0.279273  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-03-22 10:44:35,279]\u001b[0m Trial 0 failed with parameters: {'seq_length': 26, 'lag': 3, 'dif_all': False, 'TI': True, 'index': None, 'batch_size': 128} because of the following error: IndexError('Target -9223372036854775808 is out of bounds.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"c:\\Users\\karll\\Documents\\Data science\\Examensarbete\\Master-Thesis\\Optimizer\\optimizer.py\", line 224, in objective\n",
      "    learn.fit_one_cycle(epochs, lr_max=0.001)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\callback\\schedule.py\", line 119, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 264, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 253, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 248, in _do_epoch\n",
      "    self._do_epoch_validate()\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 244, in _do_epoch_validate\n",
      "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 205, in all_batches\n",
      "    for o in enumerate(self.dl): self.one_batch(*o)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\tsai\\learner.py\", line 40, in one_batch\n",
      "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py\", line 219, in _do_one_batch\n",
      "    self.loss_grad = self.loss_func(self.pred, *self.yb)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\losses.py\", line 54, in __call__\n",
      "    return self.func.__call__(inp, targ.view(-1) if self.flatten else targ, **kwargs)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\losses.py\", line 205, in forward\n",
      "    return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 2688, in nll_loss\n",
      "    return handle_torch_function(\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\overrides.py\", line 1534, in handle_torch_function\n",
      "    result = torch_func_method(public_api, types, args, kwargs)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\torch_core.py\", line 372, in __torch_function__\n",
      "    res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\_tensor.py\", line 1279, in __torch_function__\n",
      "    ret = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 2701, in nll_loss\n",
      "    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "IndexError: Target -9223372036854775808 is out of bounds.\n",
      "\u001b[33m[W 2023-03-22 10:44:35,286]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target -9223372036854775808 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mOptimizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize_data_classification, optimize_data_regression\n\u001b[0;32m      3\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mData\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtwelve_data\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mAAPL_1min\u001b[39m\u001b[39m'\u001b[39m)[:\u001b[39m2000\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m optimize_data_classification(df,\u001b[39m'\u001b[39;49m\u001b[39mtwelve\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m1min\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\karll\\Documents\\Data science\\Examensarbete\\Master-Thesis\\Optimizer\\optimizer.py:230\u001b[0m, in \u001b[0;36moptimize_data_classification\u001b[1;34m(df, dataset, timestep, buckets, epochs, trials)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m learn\u001b[39m.\u001b[39mrecorder\u001b[39m.\u001b[39mvalues[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m2\u001b[39m] \n\u001b[0;32m    229\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 230\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mtrials)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[1;32mc:\\Users\\karll\\Documents\\Data science\\Examensarbete\\Master-Thesis\\Optimizer\\optimizer.py:224\u001b[0m, in \u001b[0;36moptimize_data_classification.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    221\u001b[0m learn \u001b[39m=\u001b[39m Learner(dls, model, loss_func\u001b[39m=\u001b[39mLabelSmoothingCrossEntropyFlat(), metrics\u001b[39m=\u001b[39m[RocAucBinary(), accuracy])\n\u001b[0;32m    223\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers([learn\u001b[39m.\u001b[39mno_logging(), learn\u001b[39m.\u001b[39mno_bar()]): \u001b[39m# [Optional] this prevents fastai from printing anything during training\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     learn\u001b[39m.\u001b[39;49mfit_one_cycle(epochs, lr_max\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n\u001b[0;32m    226\u001b[0m \u001b[39m# Return the objective value\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m learn\u001b[39m.\u001b[39mrecorder\u001b[39m.\u001b[39mvalues[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\callback\\schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[1;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[0;32m    116\u001b[0m lr_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([h[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers])\n\u001b[0;32m    117\u001b[0m scheds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[39m/\u001b[39mdiv, lr_max, lr_max\u001b[39m/\u001b[39mdiv_final),\n\u001b[0;32m    118\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmom\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, \u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoms \u001b[39mif\u001b[39;00m moms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m moms))}\n\u001b[1;32m--> 119\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mParamScheduler(scheds)\u001b[39m+\u001b[39;49mL(cbs), reset_opt\u001b[39m=\u001b[39;49mreset_opt, wd\u001b[39m=\u001b[39;49mwd, start_epoch\u001b[39m=\u001b[39;49mstart_epoch)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[1;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[0;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[1;32m--> 264\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[0;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[0;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[1;32m--> 253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[0;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:248\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_train()\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_validate()\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:244\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[1;34m(self, ds_idx, dl)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39mif\u001b[39;00m dl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: dl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls[ds_idx]\n\u001b[0;32m    243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m dl\n\u001b[1;32m--> 244\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mvalidate\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelValidException)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[0;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\tsai\\learner.py:40\u001b[0m, in \u001b[0;36mone_batch\u001b[1;34m(self, i, b)\u001b[0m\n\u001b[0;32m     38\u001b[0m b_on_device \u001b[39m=\u001b[39m to_device(b, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mdevice) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m b\n\u001b[0;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b_on_device)\n\u001b[1;32m---> 40\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[0;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\learner.py:219\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_pred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb):\n\u001b[1;32m--> 219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49myb)\n\u001b[0;32m    220\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_grad\u001b[39m.\u001b[39mclone()\n\u001b[0;32m    221\u001b[0m \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\losses.py:54\u001b[0m, in \u001b[0;36mBaseLoss.__call__\u001b[1;34m(self, inp, targ, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mif\u001b[39;00m targ\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m [torch\u001b[39m.\u001b[39mint8, torch\u001b[39m.\u001b[39mint16, torch\u001b[39m.\u001b[39mint32]: targ \u001b[39m=\u001b[39m targ\u001b[39m.\u001b[39mlong()\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten: inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,inp\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_2d \u001b[39melse\u001b[39;00m inp\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inp, targ\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten \u001b[39melse\u001b[39;00m targ, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\losses.py:205\u001b[0m, in \u001b[0;36mLabelSmoothingCrossEntropy.forward\u001b[1;34m(self, output, target)\u001b[0m\n\u001b[0;32m    203\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mlog_preds\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#We divide by that size at the return line so sum and not mean\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m:  loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\n\u001b[1;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps\u001b[39m/\u001b[39mc \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps) \u001b[39m*\u001b[39m F\u001b[39m.\u001b[39;49mnll_loss(log_preds, target\u001b[39m.\u001b[39;49mlong(), weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:2688\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2646\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"The negative log likelihood loss.\u001b[39;00m\n\u001b[0;32m   2647\u001b[0m \n\u001b[0;32m   2648\u001b[0m \u001b[39mSee :class:`~torch.nn.NLLLoss` for details.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2685\u001b[0m \u001b[39m    >>> output.backward()\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2687\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target, weight):\n\u001b[1;32m-> 2688\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2689\u001b[0m         nll_loss,\n\u001b[0;32m   2690\u001b[0m         (\u001b[39minput\u001b[39;49m, target, weight),\n\u001b[0;32m   2691\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   2692\u001b[0m         target,\n\u001b[0;32m   2693\u001b[0m         weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m   2694\u001b[0m         size_average\u001b[39m=\u001b[39;49msize_average,\n\u001b[0;32m   2695\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m   2696\u001b[0m         reduce\u001b[39m=\u001b[39;49mreduce,\n\u001b[0;32m   2697\u001b[0m         reduction\u001b[39m=\u001b[39;49mreduction,\n\u001b[0;32m   2698\u001b[0m     )\n\u001b[0;32m   2699\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2700\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1530\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m   1532\u001b[0m \u001b[39m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[0;32m   1533\u001b[0m \u001b[39m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[1;32m-> 1534\u001b[0m result \u001b[39m=\u001b[39m torch_func_method(public_api, types, args, kwargs)\n\u001b[0;32m   1536\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\fastai\\torch_core.py:372\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[0;32m    371\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[1;32m--> 372\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[0;32m    373\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[0;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:1279\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m   1278\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunction():\n\u001b[1;32m-> 1279\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1280\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[0;32m   1281\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\karll\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:2701\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2699\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2700\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2701\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target -9223372036854775808 is out of bounds."
     ]
    }
   ],
   "source": [
    "from Optimizer.optimizer import optimize_data_classification, optimize_data_regression\n",
    "\n",
    "df = pd.read_csv('Data\\\\twelve_data\\AAPL_1min')[:2000]\n",
    "optimize_data_classification(df,'twelve','1min',2,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_length': 200,\n",
       " 'batch_size': 16,\n",
       " 'hidden_size': 25,\n",
       " 'n_layers': 1,\n",
       " 'rnn_dropout': 0.5,\n",
       " 'fc_dropout': 0.4,\n",
       " 'learning_rate': 0.0019302736528178678}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'lstm_class'\n",
    "\n",
    "with open(f\"models/{model_type}/{model_type}_best_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a stock for Machine Learning Model Training and preprocesses it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label  Count  Train count  Test count  Bucket min  Bucket max\n",
      "0    0.0    432          338          93     -1.0488     -0.0001\n",
      "1    1.0    566          460         106      0.0000      0.9588\n",
      "2  Total    998          798         199     -1.0488      0.9588\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data using a custom function and split it into training and testing sets\n",
    "# Only the training and testing sets are used, so the third variable (a scalar) is discarded using an underscore\n",
    "\n",
    "seq_length = best_params.pop('seq_length', None)\n",
    "data_train, data_test, _ = preprocessing(**preprocessing_params, sequence_length=seq_length, print_info=True)\n",
    "\n",
    "# Changes the data into features and labels with the split used later in TSAI for modelling\n",
    "X, y, splits = combine_split_data([data_train[0], data_test[0]],[data_train[1], data_test[1]])\n",
    "\n",
    "# Utilizes the GPU if possible\n",
    "if torch.cuda.is_available(): X, y = X.cuda(), y.cuda()\n",
    "\n",
    "batch_size = best_params.pop('batch_size', None)\n",
    "dsets = TSDatasets(X, y, splits=splits)\n",
    "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=batch_size)\n",
    "\n",
    "# Note this tabel is before sequenceing so the actuall values is total - sequence then times train_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializes the models and learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_params \u001b[39m=\u001b[39m {key: value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m best_params\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mseq_length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m)}\n\u001b[0;32m      6\u001b[0m \u001b[39m# Initiates the models\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model_lstm_fcn \u001b[39m=\u001b[39m LSTM_FCNPlus(c_in\u001b[39m=\u001b[39mnr_features, c_out\u001b[39m=\u001b[39mnr_labels, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[39m#model_lstm = LSTMPlus(c_in=nr_features, c_out=nr_labels, seq_len=sequence_length)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m#model_tst = TST(c_in=nr_features, c_out=nr_labels, seq_len=sequence_length)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m#model_xcm = XCMPlus(c_in=nr_features, c_out=nr_labels, seq_len=sequence_length)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m models \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mLSTM_FCNPlus\u001b[39m\u001b[39m'\u001b[39m: model_lstm_fcn}\u001b[39m#, 'LSTMPlus': model_lstm, 'TST': model_tst, 'XCMPlus': model_xcm}\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_layers'"
     ]
    }
   ],
   "source": [
    "nr_features = X.shape[1] # Number of features\n",
    "nr_labels = torch.unique(y).numel() # Number of labels\n",
    "\n",
    "model_params = {key: value for key, value in best_params.items() if key not in ('seq_length', 'batch_size', 'learning_rate')}\n",
    "\n",
    "# Initiates the models\n",
    "model_lstm_fcn = LSTM_FCNPlus(c_in=nr_features, c_out=nr_labels, **model_params, shuffle=False)\n",
    "#model_lstm = LSTMPlus(c_in=nr_features, c_out=nr_labels, seq_len=sequence_length)\n",
    "#model_tst = TST(c_in=nr_features, c_out=nr_labels, seq_len=sequence_length)\n",
    "#model_xcm = XCMPlus(c_in=nr_features, c_out=nr_labels, seq_len=sequence_length)\n",
    "\n",
    "models = {'LSTM_FCNPlus': model_lstm_fcn}#, 'LSTMPlus': model_lstm, 'TST': model_tst, 'XCMPlus': model_xcm}\n",
    "\n",
    "# Create Learner objects\n",
    "binary_classification_metrics = [accuracy]\n",
    "\n",
    "learn_lstm_fcn = Learner(dls, model_lstm_fcn, loss_func=LabelSmoothingCrossEntropyFlat(), metrics=accuracy)\n",
    "#learn_lstm = Learner(dls, model_lstm, loss_func=LabelSmoothingCrossEntropyFlat())\n",
    "#learn_tst = Learner(dls, model_tst, loss_func=LabelSmoothingCrossEntropyFlat())\n",
    "#learn_xcm = Learner(dls, model_xcm, loss_func=LabelSmoothingCrossEntropyFlat())\n",
    "\n",
    "learners = {'LSTM_FCNPlus': learn_lstm_fcn}#, 'LSTMPlus': learn_lstm, 'TST': learn_tst, 'XCMPlus': learn_xcm}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimal learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [0.7115873694419861,0.5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Model Name', 'Rate', 'Accuracy', 'Training Time'])\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "learning_rate = best_params.pop('learning_rate', None)\n",
    "\n",
    "\n",
    "for name, learner in learners.items():\n",
    "    start_time = time.time()\n",
    "    learner.fit_one_cycle(n_epoch=epochs, lr_max=learning_rate)\n",
    "    end_time = time.time()\n",
    "    training_time = round(end_time - start_time, 2)\n",
    "\n",
    "    loss = learner.validate()\n",
    "\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68687a499e0775bfb06d4a0c91954dfe7dcdc7f84c6d272d4d4615fc1c039e3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
