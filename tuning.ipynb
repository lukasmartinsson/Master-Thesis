{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from alpaca.data.historical import CryptoHistoricalDataClient, StockHistoricalDataClient\n",
    "\n",
    "#from Data.historical_data import Historical_data\n",
    "import pandas as pd\n",
    "from Preprocessing.preprocessing import preprocessing\n",
    "from Models.LSTM.lightningLSTM import LightningLSTM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from Preprocessing.dataclasses import StockPriceDataModule, StockDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, StochasticWeightAveraging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from hyperopt import fmin, tpe, hp\n",
    "import time\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data\\Stock\\StockBars\\MSFT_Minute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best_checkpoint3\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "#logger = TensorBoardLogger(\"logs/\", name=\"lightninglstm\")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "\n",
    "progressbar = TQDMProgressBar(refresh_rate=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [03:00<57:13, 180.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | LSTM    | 257 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "257 K     Trainable params\n",
      "0         Non-trainable params\n",
      "257 K     Total params\n",
      "1.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0e0712d43744fc98f1f071bdf4085f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b188dd54f74d4dca874ba5c7919b2909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f18be742f6e477e8a8cf2ea59d6c083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c8fd59fba24ecb8fea3948af9ed796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7499390840530396     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7499390840530396    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test_loss': 1.7499390840530396}]                   \n",
      "  0%|          | 0/20 [01:04<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                   job exception: dictionary update sequence element #0 has length 1; 2 is required\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [01:04<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# Perform the hyperparameter search using the TPE algorithm\u001b[39;00m\n\u001b[0;32m     65\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39mmax_evals)\n\u001b[1;32m---> 67\u001b[0m best \u001b[39m=\u001b[39m fmin(fn\u001b[39m=\u001b[39;49mobjective, space\u001b[39m=\u001b[39;49mspace, algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest, max_evals\u001b[39m=\u001b[39;49mmax_evals, rstate\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mdefault_rng (\u001b[39m42\u001b[39;49m))\n\u001b[0;32m     69\u001b[0m pbar\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     71\u001b[0m best\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\envs\\ai\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\envs\\ai\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\envs\\ai\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[0;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\envs\\ai\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[0;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\envs\\ai\\lib\\site-packages\\hyperopt\\base.py:897\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n\u001b[0;32m    896\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 897\u001b[0m     dict_rval \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(rval)\n\u001b[0;32m    898\u001b[0m     status \u001b[39m=\u001b[39m dict_rval[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    899\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m STATUS_STRINGS:\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Set the hyperparameters, batch size, and sequence length\n",
    "    LEARNING_RATE = params['learning_rate']\n",
    "    BATCH_SIZE = params['batch_size']\n",
    "    SEQUENCE_LENGTH = params['sequence_length']\n",
    "    HIDDEN_SIZE = params['hidden_size']\n",
    "    NUM_LAYERS = params['num_layers']\n",
    "    \n",
    "    data_train, data_test, _ = preprocessing(df=df, lag=1, sequence_length=SEQUENCE_LENGTH, dif_all=True, train_size=0.99)\n",
    "\n",
    "    # Create the data module with the given batch size and sequence length\n",
    "    data_module = StockPriceDataModule(train_sequence=data_train, test_sequence=data_test, batch_size=BATCH_SIZE, num_workers=4)\n",
    "    data_module.setup()\n",
    "\n",
    "    # Create the Lightning module with the given hyperparameters\n",
    "    model = LightningLSTM(input_size=9, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Train the model for a fixed number of epochs and measure the training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=1, callbacks=[progressbar], accelerator=\"gpu\", devices=1)\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Evaluate the model on the test set and return the loss\n",
    "    test_loss = trainer.test(model, data_module)\n",
    "    print(test_loss)\n",
    "    # Define the column titles for the hyperparameter log\n",
    "    log_columns = ['learning_rate', 'batch_size', 'sequence_length', 'hidden_size', 'num_layers', 'test_loss', 'training_time']\n",
    "\n",
    "    # Log the performance for the current hyperparameter configuration\n",
    "    log_dict = {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'test_loss': test_loss,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "    # Save the results to a text file\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(','.join([str(log_dict[col]) for col in log_columns]) + '\\n')\n",
    "\n",
    "    # Return the test loss and set the status to 'ok'\n",
    "    return {'loss': test_loss, 'status': 'ok'}\n",
    "\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -6, -1),\n",
    "    'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),\n",
    "    'sequence_length': hp.choice('sequence_length', [32, 64, 128, 256]),  \n",
    "    'hidden_size': hp.choice('hidden_size', [64, 80, 96, 112, 128,256, 512]),\n",
    "    'num_layers': hp.choice('num_layers', [1, 2, 3])\n",
    "}\n",
    "\n",
    "# Set the maximum number of evaluations for the hyperparameter search\n",
    "max_evals = 2\n",
    "\n",
    "# Perform the hyperparameter search using the TPE algorithm\n",
    "pbar = tqdm(total=max_evals)\n",
    "\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, rstate=np.random.default_rng (42))\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68687a499e0775bfb06d4a0c91954dfe7dcdc7f84c6d272d4d4615fc1c039e3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
